---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: "Aleksander Moeslund Wael"
date: "01/10/2021"
output:
  pdf_document: default
  html_document: default
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading packages

```{r}
pacman::p_load(tidyverse, lmerTest, lme4, gridExtra, dfoptim)
```

# Exercise 1

## 1. 1. Creating a data frame with all subject data

```{r}
files <- list.files(path = "experiment_2",         
                    pattern = ".csv",
                    full.names = T)

df <- read_csv(files)
```
  
## 1. 2. Describing the data
  
The dataset contains 18131 observations described by 17 variables. Data from 29 subjects is included. Thorough description in next task.

### 1. 2. i.
  
Adding variable "correct" to display if subject was correct
  
```{r}
# Adding empty variable
df <- df %>% 
  mutate(obj.resp.2 = obj.resp)

# Renaming rows in obj.resp.2 to get same units as target.type
df$obj.resp.2 <- replace(df$obj.resp.2, df$obj.resp.2 == "e", "even")
df$obj.resp.2 <- replace(df$obj.resp.2, df$obj.resp.2 == "o", "odd")

# Adding value for correct and incorrect answers
df_correct <- df %>%
  filter(obj.resp.2 == target.type) %>% 
  mutate(correct = "1")

# Joining with my df
df <- left_join(df, df_correct)

# Remaining are NAs, so replace with 0
df$correct <- replace(df$correct, is.na(df$correct), "0")
```

### 1. 2. ii. Describe what the following variables in the data frame contain, trial.type, pas, trial, target.contrast, cue, task, target_type, rt.subj, rt.obj, obj.resp, subject and correct. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what class they should be classified into, e.g. factor, numeric etc.

trial.type: Indicates whether subject is doing the staircase task (first experiment) or the follow-up experiment.
Should be class character, as it is a category.
pas: Indicates subjects response to trial on the Perceptual Awareness Scale (PAS). Takes a value between 1-4, and will therefore be treated as numeric.
trial: A numbered list for every trial the subject completes, i.e. presses e or o in either of the trial types., per subject. I should think character class for now (might change).
target.contrast: The contrast between the background and the digit (target). Between 0-1, treated as numeric.
cue: The specific cue pattern, will treat as character.
task: Whether cue pattern is 2 (singles), 4 (pairs) or 8 (quadruplets) digits. Will treat as character.
target.type: Whether target type is an odd or even number - will treat as character.
rt.subj: Reaction time for response to PAS pr. trail - will treat as numeric.
rt.obj: Reaction time for responding if target is even or odd - will treat as numeric.
obj.resp: Subjects response to target is either even or odd - will treat as character.
subject: Participant ID, ordered from 001. Treated as character/factor.
correct: Whether subject answered correctly in the trail, 1 for correct and 0 for incorrect. Is logical (binary), NOTE: treated as a factor due to an error when conducting analysis.

```{r}
# Assigning variables to proper class
df$pas <- as.numeric(df$pas)
df$trial <- as.character(df$trial)
df$target.contrast <- as.numeric(df$target.contrast)
df$cue <- as.character(df$cue)
df$rt.subj <- as.numeric(df$rt.subj)
df$rt.obj <- as.numeric(df$rt.obj)
df$target.contrast <- as.numeric(df$target.contrast)
df$correct <- as.factor(df$correct)
df$subject <- as.factor(df$subject)
```

### 1. 2. iii. for the staircasing part only, create a plot for each subject where you plot the estimated function (on the target.contrast range from 0-1) based on the fitted values of a model (use glm) that models correct as dependent on target.contrast. These plots will be our no-pooling model. Comment on the fits - do we have enough data to plot the logistic functions?

```{r}
# Making a df
staircase <- df %>% 
  filter(df$trial.type == 'staircase')

# Making a function to run a model for each participant
nopoolfun <- function(i){
  dat <- staircase[which(staircase$subject == i),] # subsetting the data so it only includes one participant
  model <- glm(correct ~ target.contrast, family = 'binomial', data = dat) # running a model on the data from one participant
  fitted <- model$fitted.values # extracting the fitted values
  plot_dat <- data.frame(cbind(fitted, 'target.contrast' = dat$target.contrast)) # creating a data frame with the variables needed in the plot
  
  plot <- ggplot(plot_dat, aes(x = target.contrast, y = fitted))+ # plotting
    geom_point(color = 'steelblue') +
    xlab('Target Contrast') +
    ylab('Predicted') +
    ylim(c(0,1))+
    ggtitle(paste0('Participant ', as.character(i))) +
    theme_minimal() +
    theme(plot.title = element_text(size = 10), axis.title=element_text(size = 8), axis.text=element_text(size=6))
  
  return(plot)
}

# Running the function for every participant (doing it twice so the plots are nicer to look at)
subjects <- c("001", "002", "003", "004", "005", "006", "007", "008", "009", "010", "011", "012", "013", "014", "015", "016")
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)
subjects <- c("017", "018", "019", "020", "021", "022", "023", "024", "025", "026", "027", "028", "029")
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)
```

I should think we have enough data, yes, although no pooling analysis does not inform us about the population.

### 1. 2. iv. on top of those plots, add the estimated functions (on the target.contrast range from 0-1) for each subject based on partial pooling model (use glmer from the package lme4) where unique intercepts and slopes for target.contrast are modelled for each subject

```{r}
# Making a df
staircase <- df %>% 
  filter(df$trial.type == 'staircase')

# Model with random intercepts and slopes (partial pooling)
m2 <- glmer(correct ~ target.contrast + (1 + target.contrast | subject), family = 'binomial', data = staircase)

# Making a function to run a model for each participant
partialpoolfun <- function(i){
  dat <- staircase[which(staircase$subject == i),] # subsetting the data so it only includes one participant
  model <- glm(correct ~ target.contrast, family = 'binomial', data = dat) # running a model on the data from one participant
  fitted <- model$fitted.values # extracting the fitted values
  plot_dat <- data.frame(cbind(fitted, 'target.contrast' = dat$target.contrast)) # creating a data frame with the variables needed in the plot
  fitted2 <- fitted.values(m2) # Adding fitted values for partial pooling model
  plot_dat_2 <- staircase %>% # Subsetting this df also pr subject
    mutate("fitted.values" = fitted2) %>% 
    filter(subject == i)
  
  plot <- ggplot(plot_dat, aes(x = target.contrast, y = fitted))+ # plotting
    geom_point(color = 'steelblue') +
    geom_line(data = plot_dat_2, aes(x = target.contrast, y = fitted.values)) + # THIS IS THE PARTIAL POOLING MODEL
    xlab('Target Contrast') +
    ylab('Predicted') +
    ylim(c(0,1))+
    ggtitle(paste0('Participant ', as.character(i))) +
    theme_minimal() +
    theme(plot.title = element_text(size = 10), axis.title=element_text(size = 8), axis.text=element_text(size=6))
  
  return(plot)
}

# Running the function for every participant
subjects <- c("001", "002", "003", "004", "005", "006", "007", "008", "009", "010", "011", "012", "013", "014", "015", "016")
plots <- lapply(subjects, FUN=partialpoolfun)
do.call(grid.arrange,  plots)
subjects <- c("017", "018", "019", "020", "021", "022", "023", "024", "025", "026", "027", "028", "029")
plots <- lapply(subjects, FUN=partialpoolfun)
do.call(grid.arrange,  plots)
```

### 1. 2. v. in your own words, describe how the partial pooling model allows for a better fit for each subject

Partial pooling allows for the model to be generalizable (i.e. "less accurate" fit compared to no pooling), but still accounts for subject differences in baseline (intercept) and performance (slopes).

# Exercise 2

```{r}
# Making df
df_experiment <- df %>% 
  filter(trial.type == "experiment")
```

### 2. 1. Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (rt.obj) based on a model where only intercept is modelled

```{r}
# Modelling simple response time
response_time <- lm(rt.obj ~ 1, data = df_experiment)
df_experiment$fitted_rt <- fitted(response_time)
```

```{r}
# Subsetting for each subject
subject1 <- df_experiment %>% 
  filter(subject == "001")
subject2 <- df_experiment %>% 
  filter(subject == "002")
subject3 <- df_experiment %>% 
  filter(subject == "003")
subject4 <- df_experiment %>% 
  filter(subject == "004")

# Modelling each subject 
interceptmodel1 <- lm(rt.obj ~ 1, data = subject1)
interceptmodel2 <- lm(rt.obj ~ 1, data = subject2)
interceptmodel3 <- lm(rt.obj ~ 1, data = subject3)
interceptmodel4 <- lm(rt.obj ~ 1, data = subject4)

# Plotting
car::qqPlot(interceptmodel1)
car::qqPlot(interceptmodel2)
car::qqPlot(interceptmodel3)
car::qqPlot(interceptmodel4)
```

### 2. 1. i. comment on these

They are all somewhat normally distributed, but not satisfactory as there is heavy skewness in some of the plots and some heavy outliers in the top end mainly.

### 2. 1. ii. does a log-transformation of the response time data improve the Q-Q-plots?
```{r}
# Log transforming for each subject
logsubject1 <- subject1 %>% 
  mutate(log_rt = log(rt.obj))
logsubject2 <- subject2 %>% 
  mutate(log_rt = log(rt.obj))
logsubject3 <- subject3 %>% 
  mutate(log_rt = log(rt.obj))
logsubject4 <- subject4 %>% 
  mutate(log_rt = log(rt.obj))

# Modelling
loginterceptmodel1 <- lm(log_rt ~ 1, data = logsubject1)
loginterceptmodel2 <- lm(log_rt ~ 1, data = logsubject2)
loginterceptmodel3 <- lm(log_rt ~ 1, data = logsubject3)
loginterceptmodel4 <- lm(log_rt ~ 1, data = logsubject4)

# Plotting
car::qqPlot(loginterceptmodel1)
car::qqPlot(loginterceptmodel2)
car::qqPlot(loginterceptmodel3)
car::qqPlot(loginterceptmodel4)
```

I would say the log transformation reduced the tail sizes (outliers), which betters the distribution for subject 1 and 2. Though distribution for subject 3 is now less optimal.

### 2. 2. Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification) 
```{r}
rt_partialpooling1 <- lmer(rt.obj ~ task + (1|subject), REML = FALSE, data = df_experiment)
rt_partialpooling2 <- lmer(rt.obj ~ task + (1|subject) + (1|trial), REML = FALSE, data = df_experiment)
```

### 2. 2. i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  

```{r}
rt_no_ranef <- lm(rt.obj ~ task, data = df_experiment)
anova(rt_partialpooling1, rt_partialpooling2, rt_no_ranef)
summary(rt_partialpooling1)
```

Optimally, i should like to model the hierarchies i'd expect exist in the data. That would definitively include random intercepts per subject, as I assume reaction time is in some sense dependent on subject. Although this doesn't explain much of the variance, it does reduce the AIC slightly. I would probably also consider doing random intercepts for trial, but seeing as this increases the AIC value, i don't regard it as the right choice, as i only want to add terms that increase my models explanatory power.

### 2. 2. ii. explain in your own words what your chosen models says about response times between the different tasks  

My model shows that task significantly predicts reaction time for all three tasks (p < 0.05). Subjects have highest reaction time with doubles, then quadruplets, then singles. 

### 2. 3. Now add _pas_ and its interaction with _task_ to the fixed effects 
```{r}
rt_partialpooling3 <- lmer(rt.obj ~ task*pas + (1|subject), REML = FALSE, data = df_experiment)
summary(rt_partialpooling3)
```

### 2. 3. i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?
```{r}
rt_partialpooling4 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial), REML = FALSE, data = df_experiment)
rt_partialpooling5 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit), REML = FALSE, data = df_experiment)
rt_partialpooling6 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue), REML = FALSE, data = df_experiment)
rt_partialpooling7 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue) + (1|pas), REML = FALSE, data = df_experiment)
rt_partialpooling8 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue) + (1|pas) + (1|seed), REML = FALSE, data = df_experiment)
rt_partialpooling9 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue) + (1|pas) + (1|seed) + (1|even.digit), REML = FALSE, data = df_experiment) # Only this one failed to converge!
```

### 2. 3. ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)

```{r}
print(VarCorr(rt_partialpooling9), comp='Variance')
?isSingular
```

The fit is singular because "even.digit" explains no variance, i.e. is estimated at exactly 0.

### 2. 3. iii. in your own words - how could you explain why your model would result in a singular fit?

If you add two variables that explain exactly the same variance, you would end up with a singular fit. So optimally you want to add terms that explain a lot of variance of which no other variables explain.

# Exercise 3

```{r}
# Making df
data.count <- df %>%
  group_by(subject, task, pas) %>%
  dplyr::summarise("count" = n())
```        

### 3. 2. Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled  

```{r}
pasmodel <- glmer(count ~ pas*task + (pas|subject), data = data.count, family = poisson, control = glmerControl(optimizer="bobyqa"))
summary(pasmodel)
```

### 3. 2. i. which family should be used?

Poisson, because it's good for modelling frequency (count). 

### 3. 2. ii. why is a slope for _pas_ not really being modelled?  

Because pas isn't continuous, we can't model a proper slope, so this is kind of a "pseudo"-slope.

### 3. 2. iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)

I did get the error, now it works :) 

### 3. 2. iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction

Did not have a converging fit, but will do so anyway.

```{r}
pasmodel2 <- glmer(count ~ pas + task + (pas|subject), data = data.count, family = poisson)
summary(pasmodel2)
```

The interaction model does model the data better with all interaction effects being significant (p < 0.05), and with a lower AIC value.

### 3. 2. v. indicate which of the two models, you would choose and why  

I would choose the model with the interaction effect included. This seems a too important distinction in the data to be ignored.

### 3. 2. vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  

```{r}
tibble("SSR interaction model" = sum(residuals(pasmodel)^2), "SSR no interaction" = sum(residuals(pasmodel2)^2))
AIC(pasmodel, pasmodel2)
```



### 3. 2. vii.

```{r}
# Subsetting
pas_foursubjects <- data.count %>% 
  filter(subject == "001"|subject == "002"|subject == "003"|subject == "004")

# Modelling
pasmodel_foursubjects <- glmer(count ~ pas*task + (pas|subject), data = pas_foursubjects, family = poisson)

# Plotting
pas_foursubjects %>% 
  ggplot() +
  geom_point(aes(x = pas, y = fitted(pasmodel_foursubjects), color = "Estimated")) +
  geom_point(aes(x = pas, y = count, color = "Observed")) +
  facet_wrap( ~ subject)
```


### 3. 3. Finally, fit a multilevel model that models correct as dependent on task with a unique intercept for each subject

```{r}
df_end_me <- glmer(correct ~ task + (1 | subject), data = df, family = "binomial")
```

### 3. 3. i. does task explain performance?

```{r}
summary(df_end_me)
```

Task significantly predicts correctness for all task levels (all p < 0.05).

### 3. 3. ii. add pas as a main effect on top of task - what are the consequences of that?

```{r}
df_end_me_more <- glmer(correct ~ task + pas + (1 | subject), data = df, family = "binomial")
summary(df_end_me_more)
```

Since task is no longer significant, it seems that pas explains more of the variance, i.e. a better predictor.

### 3. 3. iii. now fit a multilevel model that models correct as dependent on pas with a unique intercept for each subject

```{r}
df_end_me_more_now <- glmer(correct ~ pas + (1 | subject), data = df, family = "binomial")
summary(df_end_me_more_now)
```

### 3. 3. iv. finally, fit a model that models the interaction between task and pas and their main effects

```{r}
df_end_me_more_now_pls <- glm(correct ~ task * pas, data = df, family = "binomial")
summary(df_end_me_more_now_pls)
```

### 3. 3. v. describe in your words which model is the best in explaining the variance in accuracy.

The model which predicts correct by pas with intercepts per subject has the lowest AIC value and a significant chi-square value. This model strikes the balance between complexity and explanatory power.

